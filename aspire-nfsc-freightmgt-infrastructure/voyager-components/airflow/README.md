# Example Component

Airflow Component for running an Airflow deployment

## Example Usage

cd into example_component
run setup.sh with required para's

```
cd example_component
bash setup.sh <TF_STATE_BUCKET> <TF_STATE_DDB_TABLE> <TF_STATE_S3_KEY> <TF_VAR_region> <TFVARS_PATH> <TF_VAR_environment>
```

<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->
## Requirements

All Python requirements should be in requirments.txt file

## Providers

Providers required and versions

| Name | Version |
|------|---------|
| aws | 1.0 |


## TFVAR Inputs

| Name | Description | Type | Default | Required |
|------|-------------|------|---------|:--------:|
| airflow_region | The region to deploy your solution to | `string` | `"eu-west-1"` | no |
| resource_prefix | A prefix for the create resources, example your company name (be aware of the resource name length) | `string` | `"voyager"` | no |
| resource_suffix | A suffix for the created resources, example the environment for airflow to run in (be aware of the resource name length) | `string` | `"dev"` | yes |
| extra_tags | Extra tags that you would like to add to all created resources | `map(string)` | `{}` | no |
| airflow_image_name | The name of the airflow image | `string` | `"apache/airflow"` | no |
| airflow_image_tag | The tag of the airflow image | `string` | `"1.10.12"` | no |
| airflow_executor | Example Input 3 | `string` | `"Local"` | no |
| airflow_authentication | Authentication backend to be used, supported backends [\"\", \"rbac\"] | `string` | `""` | no |
| airflow_py_requirements_path | The relative path to a python requirements.txt file to install extra packages in the container that you can use in your DAGs. | `string` | `"./requirements.txt"` | no |
| airflow_variables | The variables passed to airflow as an environment variable (see airflow docs for more info https://airflow.apache.org/docs/). You can not specify \"AIRFLOW__CORE__SQL_ALCHEMY_CONN\" and \"AIRFLOW__CORE__EXECUTOR\" (managed by this module) | `string` | `{AIRFLOW__WEBSERVER__NAVBAR_COLOR : "#f58220"}` | no |
| airflow_container_home | Working dir for airflow (only change if you are using a different image) | `string` | `"/opt/airflow"` | no |
| airflow_log_region | The region you want your airflow logs in, defaults to the region variable | `string` | `""` | no |
| airflow_log_retention | The number of days you want to keep the log of airflow container | `string` | `"7"` | no |
| airflow_example_dag | Add an example dag on startup (mostly for sanity check) | `bool` | `true` | no |
| ecs_cpu | The allocated cpu for your airflow instance | `number` | `1024` | no |
| ecs_memory | The allocated memory for your airflow instance | `number` | `2048` | no |
| ip_allow_list | A list of ip ranges that are allowed to access the airflow webserver, default: full access | `list(string)` | `["109.94.138.30/32", "109.94.137.128/32", "109.94.137.129/32", "109.94.137.130/32", "109.94.138.128/32", "109.94.138.129/32", "109.94.138.130/32", "46.233.117.136/29" , "46.233.117.152/29"]` | no |
| vpc_id | The id of the vpc where you will run ECS/RDS | `string` | `""` | yes |
| public_subnet_ids | A list of subnet ids of where the ALB will reside, if the \"private_subnet_ids\" variable is not provided ECS and RDS will also reside in these subnets | `list(string)` | `[]` | no |
| private_subnet_ids | A list of subnet ids of where the ECS and RDS reside, this will only work if you have a NAT Gateway in your VPC | `list(string)` | `[]` | no |
| use_https | Expose traffic using HTTPS or not | `bool` | `false` | no |
| dns_name | The DNS name that will be used to expose Airflow. Optional if not serving over HTTPS. Will be autogenerated if not provided | `string` | `""` | no |
| certificate_arn | The ARN of the certificate that will be used | `string` | `""` | no |
| route53_zone_name | The name of a Route53 zone that will be used for the certificate validation. | `string` | `""` | no |
| postgres_uri | The postgres uri of your postgres db, if none provided a postgres db in rds is made. Format \"<db_username>:<db_password>@<db_endpoint>:<db_port>/<db_name>\" | `string` | `""` | no |
| rds_username | Username of rds | `string` | `"airflow"` | no |
| rds_password | Password of rds | `string` | `""` | no |
| rds_instance_class | The class of instance you want to give to your rds db | `string` | `"db.t2.micro"` | no |
| rds_availability_zone | Availability zone for the rds instance | `string` | `"eu-west-1a"` | no |
| rds_skip_final_snapshot | Whether or not to skip the final snapshot before deleting (mainly for tests) | `bool` | `false` | no |
| rds_deletion_protection | Deletion protection for the rds instance | `string` | `false` | no |
| s3_bucket_name | The S3 bucket name where the DAGs and startup scripts will be stored, leave this blank to let this module create a s3 bucket for you. WARNING: this module will put files into the path \"dags/\" and \"startup/\" of the bucket | `string` | `""` | no |

## Outputs

| Name | Description |
|------|-------------|
| airflow_alb_dns | The DNS name of the ALB, with this you can access the Airflow webserver
| airflow_connection_sg | The security group with which you can connect other instance to Airflow, for example EMR Livy
| airflow_dns_record | The created DNS record (only if "use_https" = true)
| airflow_task_iam_role | The IAM role of the airflow task, use this to give Airflow more permissions
<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->